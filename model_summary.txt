Quantizing model (QAT)...
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
         QuantStub-1          [-1, 3, 224, 224]               0
            Conv2d-2         [-1, 64, 112, 112]           9,408
       BatchNorm2d-3         [-1, 64, 112, 112]             128
              ReLU-4         [-1, 64, 112, 112]               0
         MaxPool2d-5           [-1, 64, 56, 56]               0
            Conv2d-6           [-1, 64, 56, 56]           4,096
       BatchNorm2d-7           [-1, 64, 56, 56]             128
              ReLU-8           [-1, 64, 56, 56]               0
            Conv2d-9           [-1, 64, 56, 56]          36,864
      BatchNorm2d-10           [-1, 64, 56, 56]             128
             ReLU-11           [-1, 64, 56, 56]               0
           Conv2d-12          [-1, 256, 56, 56]          16,384
      BatchNorm2d-13          [-1, 256, 56, 56]             512
           Conv2d-14          [-1, 256, 56, 56]          16,384
      BatchNorm2d-15          [-1, 256, 56, 56]             512
         Identity-16          [-1, 256, 56, 56]               0
QuantizableBottleneck-17          [-1, 256, 56, 56]               0
           Conv2d-18           [-1, 64, 56, 56]          16,384
      BatchNorm2d-19           [-1, 64, 56, 56]             128
             ReLU-20           [-1, 64, 56, 56]               0
           Conv2d-21           [-1, 64, 56, 56]          36,864
      BatchNorm2d-22           [-1, 64, 56, 56]             128
             ReLU-23           [-1, 64, 56, 56]               0
           Conv2d-24          [-1, 256, 56, 56]          16,384
      BatchNorm2d-25          [-1, 256, 56, 56]             512
         Identity-26          [-1, 256, 56, 56]               0
QuantizableBottleneck-27          [-1, 256, 56, 56]               0
           Conv2d-28           [-1, 64, 56, 56]          16,384
      BatchNorm2d-29           [-1, 64, 56, 56]             128
             ReLU-30           [-1, 64, 56, 56]               0
           Conv2d-31           [-1, 64, 56, 56]          36,864
      BatchNorm2d-32           [-1, 64, 56, 56]             128
             ReLU-33           [-1, 64, 56, 56]               0
           Conv2d-34          [-1, 256, 56, 56]          16,384
      BatchNorm2d-35          [-1, 256, 56, 56]             512
         Identity-36          [-1, 256, 56, 56]               0
QuantizableBottleneck-37          [-1, 256, 56, 56]               0
           Conv2d-38          [-1, 128, 56, 56]          32,768
      BatchNorm2d-39          [-1, 128, 56, 56]             256
             ReLU-40          [-1, 128, 56, 56]               0
           Conv2d-41          [-1, 128, 28, 28]         147,456
      BatchNorm2d-42          [-1, 128, 28, 28]             256
             ReLU-43          [-1, 128, 28, 28]               0
           Conv2d-44          [-1, 512, 28, 28]          65,536
      BatchNorm2d-45          [-1, 512, 28, 28]           1,024
           Conv2d-46          [-1, 512, 28, 28]         131,072
      BatchNorm2d-47          [-1, 512, 28, 28]           1,024
         Identity-48          [-1, 512, 28, 28]               0
QuantizableBottleneck-49          [-1, 512, 28, 28]               0
           Conv2d-50          [-1, 128, 28, 28]          65,536
      BatchNorm2d-51          [-1, 128, 28, 28]             256
             ReLU-52          [-1, 128, 28, 28]               0
           Conv2d-53          [-1, 128, 28, 28]         147,456
      BatchNorm2d-54          [-1, 128, 28, 28]             256
             ReLU-55          [-1, 128, 28, 28]               0
           Conv2d-56          [-1, 512, 28, 28]          65,536
      BatchNorm2d-57          [-1, 512, 28, 28]           1,024
         Identity-58          [-1, 512, 28, 28]               0
QuantizableBottleneck-59          [-1, 512, 28, 28]               0
           Conv2d-60          [-1, 128, 28, 28]          65,536
      BatchNorm2d-61          [-1, 128, 28, 28]             256
             ReLU-62          [-1, 128, 28, 28]               0
           Conv2d-63          [-1, 128, 28, 28]         147,456
      BatchNorm2d-64          [-1, 128, 28, 28]             256
             ReLU-65          [-1, 128, 28, 28]               0
           Conv2d-66          [-1, 512, 28, 28]          65,536
      BatchNorm2d-67          [-1, 512, 28, 28]           1,024
         Identity-68          [-1, 512, 28, 28]               0
QuantizableBottleneck-69          [-1, 512, 28, 28]               0
           Conv2d-70          [-1, 128, 28, 28]          65,536
      BatchNorm2d-71          [-1, 128, 28, 28]             256
             ReLU-72          [-1, 128, 28, 28]               0
           Conv2d-73          [-1, 128, 28, 28]         147,456
      BatchNorm2d-74          [-1, 128, 28, 28]             256
             ReLU-75          [-1, 128, 28, 28]               0
           Conv2d-76          [-1, 512, 28, 28]          65,536
      BatchNorm2d-77          [-1, 512, 28, 28]           1,024
         Identity-78          [-1, 512, 28, 28]               0
QuantizableBottleneck-79          [-1, 512, 28, 28]               0
           Conv2d-80          [-1, 256, 28, 28]         131,072
      BatchNorm2d-81          [-1, 256, 28, 28]             512
             ReLU-82          [-1, 256, 28, 28]               0
           Conv2d-83          [-1, 256, 14, 14]         589,824
      BatchNorm2d-84          [-1, 256, 14, 14]             512
             ReLU-85          [-1, 256, 14, 14]               0
           Conv2d-86         [-1, 1024, 14, 14]         262,144
      BatchNorm2d-87         [-1, 1024, 14, 14]           2,048
           Conv2d-88         [-1, 1024, 14, 14]         524,288
      BatchNorm2d-89         [-1, 1024, 14, 14]           2,048
         Identity-90         [-1, 1024, 14, 14]               0
QuantizableBottleneck-91         [-1, 1024, 14, 14]               0
           Conv2d-92          [-1, 256, 14, 14]         262,144
      BatchNorm2d-93          [-1, 256, 14, 14]             512
             ReLU-94          [-1, 256, 14, 14]               0
           Conv2d-95          [-1, 256, 14, 14]         589,824
      BatchNorm2d-96          [-1, 256, 14, 14]             512
             ReLU-97          [-1, 256, 14, 14]               0
           Conv2d-98         [-1, 1024, 14, 14]         262,144
      BatchNorm2d-99         [-1, 1024, 14, 14]           2,048
        Identity-100         [-1, 1024, 14, 14]               0
QuantizableBottleneck-101         [-1, 1024, 14, 14]               0
          Conv2d-102          [-1, 256, 14, 14]         262,144
     BatchNorm2d-103          [-1, 256, 14, 14]             512
            ReLU-104          [-1, 256, 14, 14]               0
          Conv2d-105          [-1, 256, 14, 14]         589,824
     BatchNorm2d-106          [-1, 256, 14, 14]             512
            ReLU-107          [-1, 256, 14, 14]               0
          Conv2d-108         [-1, 1024, 14, 14]         262,144
     BatchNorm2d-109         [-1, 1024, 14, 14]           2,048
        Identity-110         [-1, 1024, 14, 14]               0
QuantizableBottleneck-111         [-1, 1024, 14, 14]               0
          Conv2d-112          [-1, 256, 14, 14]         262,144
     BatchNorm2d-113          [-1, 256, 14, 14]             512
            ReLU-114          [-1, 256, 14, 14]               0
          Conv2d-115          [-1, 256, 14, 14]         589,824
     BatchNorm2d-116          [-1, 256, 14, 14]             512
            ReLU-117          [-1, 256, 14, 14]               0
          Conv2d-118         [-1, 1024, 14, 14]         262,144
     BatchNorm2d-119         [-1, 1024, 14, 14]           2,048
        Identity-120         [-1, 1024, 14, 14]               0
QuantizableBottleneck-121         [-1, 1024, 14, 14]               0
          Conv2d-122          [-1, 256, 14, 14]         262,144
     BatchNorm2d-123          [-1, 256, 14, 14]             512
            ReLU-124          [-1, 256, 14, 14]               0
          Conv2d-125          [-1, 256, 14, 14]         589,824
     BatchNorm2d-126          [-1, 256, 14, 14]             512
            ReLU-127          [-1, 256, 14, 14]               0
          Conv2d-128         [-1, 1024, 14, 14]         262,144
     BatchNorm2d-129         [-1, 1024, 14, 14]           2,048
        Identity-130         [-1, 1024, 14, 14]               0
QuantizableBottleneck-131         [-1, 1024, 14, 14]               0
          Conv2d-132          [-1, 256, 14, 14]         262,144
     BatchNorm2d-133          [-1, 256, 14, 14]             512
            ReLU-134          [-1, 256, 14, 14]               0
          Conv2d-135          [-1, 256, 14, 14]         589,824
     BatchNorm2d-136          [-1, 256, 14, 14]             512
            ReLU-137          [-1, 256, 14, 14]               0
          Conv2d-138         [-1, 1024, 14, 14]         262,144
     BatchNorm2d-139         [-1, 1024, 14, 14]           2,048
        Identity-140         [-1, 1024, 14, 14]               0
QuantizableBottleneck-141         [-1, 1024, 14, 14]               0
          Conv2d-142          [-1, 512, 14, 14]         524,288
     BatchNorm2d-143          [-1, 512, 14, 14]           1,024
            ReLU-144          [-1, 512, 14, 14]               0
          Conv2d-145            [-1, 512, 7, 7]       2,359,296
     BatchNorm2d-146            [-1, 512, 7, 7]           1,024
            ReLU-147            [-1, 512, 7, 7]               0
          Conv2d-148           [-1, 2048, 7, 7]       1,048,576
     BatchNorm2d-149           [-1, 2048, 7, 7]           4,096
          Conv2d-150           [-1, 2048, 7, 7]       2,097,152
     BatchNorm2d-151           [-1, 2048, 7, 7]           4,096
        Identity-152           [-1, 2048, 7, 7]               0
QuantizableBottleneck-153           [-1, 2048, 7, 7]               0
          Conv2d-154            [-1, 512, 7, 7]       1,048,576
     BatchNorm2d-155            [-1, 512, 7, 7]           1,024
            ReLU-156            [-1, 512, 7, 7]               0
          Conv2d-157            [-1, 512, 7, 7]       2,359,296
     BatchNorm2d-158            [-1, 512, 7, 7]           1,024
            ReLU-159            [-1, 512, 7, 7]               0
          Conv2d-160           [-1, 2048, 7, 7]       1,048,576
     BatchNorm2d-161           [-1, 2048, 7, 7]           4,096
        Identity-162           [-1, 2048, 7, 7]               0
QuantizableBottleneck-163           [-1, 2048, 7, 7]               0
          Conv2d-164            [-1, 512, 7, 7]       1,048,576
     BatchNorm2d-165            [-1, 512, 7, 7]           1,024
            ReLU-166            [-1, 512, 7, 7]               0
          Conv2d-167            [-1, 512, 7, 7]       2,359,296
     BatchNorm2d-168            [-1, 512, 7, 7]           1,024
            ReLU-169            [-1, 512, 7, 7]               0
          Conv2d-170           [-1, 2048, 7, 7]       1,048,576
     BatchNorm2d-171           [-1, 2048, 7, 7]           4,096
        Identity-172           [-1, 2048, 7, 7]               0
QuantizableBottleneck-173           [-1, 2048, 7, 7]               0
AdaptiveAvgPool2d-174           [-1, 2048, 1, 1]               0
          Linear-175                   [-1, 19]          38,931
     DeQuantStub-176                   [-1, 19]               0
================================================================
Total params: 23,546,963
Trainable params: 23,546,963
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.57
Forward/backward pass size (MB): 287.70
Params size (MB): 89.82
Estimated Total Size (MB): 378.10
----------------------------------------------------------------
Fused layer1.0.downsample
Fused layer2.0.downsample
Fused layer3.0.downsample
Fused layer4.0.downsample
Performing QAT-training cycles...
Epoch 1, Loss: 0.1895, Accuracy: 0.9340
Test Loss: 1.3788, Test Accuracy: 0.6115
Test Loss: 1.6439, Test Accuracy: 0.5899
